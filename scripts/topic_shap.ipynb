{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset blog_authorship_corpus (/home/steinad/.cache/huggingface/datasets/blog_authorship_corpus/blog_authorship_corpus/1.0.0/6f5d78241afd8313111956f877a57db7a0e9fc6718255dc85df0928197feb683)\n",
      "Found cached dataset blog_authorship_corpus (/home/steinad/.cache/huggingface/datasets/blog_authorship_corpus/blog_authorship_corpus/1.0.0/6f5d78241afd8313111956f877a57db7a0e9fc6718255dc85df0928197feb683)\n",
      "Loading cached processed dataset at /home/steinad/.cache/huggingface/datasets/blog_authorship_corpus/blog_authorship_corpus/1.0.0/6f5d78241afd8313111956f877a57db7a0e9fc6718255dc85df0928197feb683/cache-91bee97bd74f5a2a.arrow\n",
      "Loading cached processed dataset at /home/steinad/.cache/huggingface/datasets/blog_authorship_corpus/blog_authorship_corpus/1.0.0/6f5d78241afd8313111956f877a57db7a0e9fc6718255dc85df0928197feb683/cache-fabb96a2d990d2e2.arrow\n"
     ]
    }
   ],
   "source": [
    "config = {\"dataset\": \"blog\", \"topics\": \"lda\"}\n",
    "data_train, data_val, data_test = load_data(config)\n",
    "x = [data_test[i]['sentence'] for i in range(100)]\n",
    "# x = [xi for xi in x if len(xi.split()) > 1]\n",
    "# topics, word2idx = get_topics(config, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.83110830e-03 -2.28541243e-03  4.76596788e-03  9.03018709e-04\n",
      "   -9.72774600e-04]\n",
      "  [-4.56672795e-03 -1.23948330e-02  1.90876040e-02  1.70953730e-02\n",
      "   -1.76202119e-02]\n",
      "  [-1.00317569e-03 -1.06399252e-03  2.31555756e-03  2.07160924e-03\n",
      "   -2.09181920e-03]\n",
      "  ...\n",
      "  [-5.13386766e-03 -5.93131461e-03  1.18152253e-02  1.36682045e-02\n",
      "   -1.39388373e-02]\n",
      "  [-2.10988910e-03 -1.14157639e-03  3.66272985e-03  1.75312241e-03\n",
      "   -1.84189796e-03]\n",
      "  [-7.43208023e-02 -1.33828489e-01  2.25261041e-01  2.54432237e-01\n",
      "   -2.56774188e-01]]\n",
      "\n",
      " [[ 6.36622124e-03 -5.68307352e-03 -7.12220114e-04  8.31600828e-04\n",
      "   -6.57964214e-04]\n",
      "  [ 1.25268497e-02 -1.11453457e-02 -1.99797497e-03  3.71074187e-03\n",
      "   -3.75788151e-03]\n",
      "  [ 3.75723545e-03 -3.37631277e-03 -5.61349138e-04  7.91916432e-04\n",
      "   -8.12950315e-04]\n",
      "  ...\n",
      "  [ 1.80022132e-02 -1.59178442e-02 -3.30123717e-03  3.54622948e-03\n",
      "   -3.81129762e-03]\n",
      "  [ 5.12343988e-03 -4.40378015e-03 -1.24138711e-03  4.00002516e-04\n",
      "   -4.82361620e-04]\n",
      "  [ 4.83114505e-01 -4.34807206e-01 -7.65202824e-02  7.41111380e-02\n",
      "   -7.63404878e-02]]\n",
      "\n",
      " [[ 2.30981266e-02 -2.34813926e-02  1.74790033e-03 -1.35258116e-02\n",
      "    1.44796915e-02]\n",
      "  [ 4.95782370e-03 -3.45101694e-03 -1.77137063e-03  1.37541844e-03\n",
      "   -1.67695343e-03]\n",
      "  [ 1.63944602e-03 -1.44057183e-03 -3.60321036e-04 -4.00820506e-04\n",
      "    3.52039698e-04]\n",
      "  ...\n",
      "  [ 5.44904770e-04  2.96531724e-04 -6.56578101e-04  4.47737735e-03\n",
      "   -4.63906824e-03]\n",
      "  [ 1.41232415e-02 -1.12794341e-02 -2.72467995e-03  1.38913835e-03\n",
      "   -1.46807513e-03]\n",
      "  [ 7.97021140e-02 -7.46408220e-02 -3.89644690e-02  6.43487580e-02\n",
      "   -6.68865281e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.84579250e-02 -7.25216300e-03 -1.52043882e-02 -3.65144397e-02\n",
      "    3.53467766e-02]\n",
      "  [-1.85259019e-03  1.22574923e-03  5.20874122e-04  1.19843420e-04\n",
      "   -7.64650610e-05]\n",
      "  [-5.46988619e-02  2.66966078e-02  2.51147826e-02 -1.02318495e-01\n",
      "    1.02079587e-01]\n",
      "  ...\n",
      "  [ 1.14857389e-03 -2.28879910e-03  3.06543391e-05  3.48207632e-04\n",
      "   -1.20488332e-04]\n",
      "  [-9.99769424e-04  1.90268400e-03 -1.21115141e-03 -1.84632573e-03\n",
      "    2.06466130e-03]\n",
      "  [-5.36224111e-02  7.16132475e-02 -1.41598452e-02 -5.83119888e-02\n",
      "    5.72633088e-02]]\n",
      "\n",
      " [[ 8.89056640e-03 -5.95419346e-03 -2.41029917e-03  2.55250728e-03\n",
      "   -2.43855718e-03]\n",
      "  [ 3.86045154e-03 -2.89498903e-03 -6.34677134e-04  7.63489401e-04\n",
      "   -7.60891644e-04]\n",
      "  [ 6.80779017e-03 -6.50346905e-03  2.38249736e-04  2.19760416e-03\n",
      "   -2.20906344e-03]\n",
      "  ...\n",
      "  [ 6.11339666e-04 -1.80920443e-04 -8.82020282e-04  1.88367094e-03\n",
      "   -1.85572318e-03]\n",
      "  [ 3.21210085e-03 -2.58138409e-03 -6.19765913e-04  1.40270020e-03\n",
      "   -1.26503834e-03]\n",
      "  [ 1.13076036e-01 -2.14393450e-01  4.94506591e-02  2.34448682e-01\n",
      "   -2.38134299e-01]]\n",
      "\n",
      " [[ 1.02549090e-04 -7.59058234e-05 -2.08609164e-05  1.44911695e-04\n",
      "   -1.48504711e-04]\n",
      "  [ 1.23558610e-03 -8.94227504e-04 -2.68617910e-04  1.06731400e-03\n",
      "   -1.06895971e-03]\n",
      "  [ 8.43201116e-04 -6.34601638e-04 -1.74028891e-04  7.07998728e-04\n",
      "   -7.21538100e-04]\n",
      "  ...\n",
      "  [ 4.42599583e-03 -3.27416087e-03 -8.75582121e-04  3.54807607e-03\n",
      "   -3.59764062e-03]\n",
      "  [ 6.52897404e-03 -5.06374248e-03 -1.21870273e-03  4.34733068e-03\n",
      "   -4.41837091e-03]\n",
      "  [ 4.92248663e-01 -3.51297298e-01 -1.03219317e-01  3.83445750e-01\n",
      "   -3.75448388e-01]]]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"blog\"\n",
    "config = {\"dataset\": dataset, \"topics\": \"lda\"}\n",
    "topics, word2idx = get_topics(config, x)\n",
    "# roberta_shap_vals = load(\"shap_vals_gpt2_sst2\")\n",
    "roberta_topic_vals = load(f\"topic_vals_distilroberta_{dataset}_lda\").reshape(-1, 31, 5)\n",
    "roberta_word_vals = load(f\"word_vals_distilroberta_{dataset}_lda\")\n",
    "# gpt2_topic_vals = load(f\"topic_vals_gpt2_{dataset}_lda\").reshape(-1, 31)\n",
    "# gpt2_word_vals = load(f\"word_vals_gpt2_{dataset}_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['oh', 'day', 'happy', 'shit', 'fuck', 'fucking', 'man', 'haha', 'yeah', 'one']\n",
      "1 ['post', 'here', 'comment', 'sub', 'read', 'sorry', 'from', 'thanks', 'got', 'reddit']\n",
      "2 ['name', 'game', 'one', 'watch', 'show', 'great', 'favorite', 'play', 'fun', 'video']\n",
      "3 ['name', 'we', 'game', 'our', 'team', 'him', 'play', 'win', 'would', 'season']\n",
      "4 ['their', 'money', 'because', 'people', 'pay', 'we', 'our', 'or', 'government', 'want']\n",
      "5 ['good', \"i'm\", 'sorry', 'hope', 'glad', 'luck', 'am', \"you're\", 'best', 'hear']\n",
      "6 ['them', 'will', 'take', 'from', \"don't\", 'stop', 'please', 'yourself', 'get', 'or']\n",
      "7 ['thanks', 'thank', 'much', ':)', 'very', \"i'll\", 'really', 'appreciate', 'try', 'good']\n",
      "8 ['would', 'did', 'say', \"didn't\", 'know', 'same', 'said', 'thought', 'thing', 'were']\n",
      "9 ['people', 'who', 'their', 'them', 'other', 'because', 'name', 'these', 'there', 'by']\n",
      "10 ['would', 'one', 'those', 'only', 'an', 'we', 'get', 'also', 'which', 'why']\n",
      "11 ['up', 'down', 'when', 'back', 'there', 'go', 'out', 'his', 'end', 'over']\n",
      "12 ['people', 'who', 'being', 'women', 'by', 'men', 'their', 'an', 'how', 'because']\n",
      "13 [\"don't\", 'know', 'don', 'think', 'how', 'we', 'people', 'want', 'why', 'dont']\n",
      "14 ['got', 'his', 'were', 'would', 'had', 'get', 'could', 'him', 'off', 'into']\n",
      "15 ['them', 'get', 'off', 'go', 'too', 'we', 'some', 'or', 'into', 'eat']\n",
      "16 ['how', \"can't\", 'out', 'did', 'see', 'wait', 'does', 'why', 'get', 'else']\n",
      "17 [\"i'm\", \"you're\", \"doesn't\", 'being', 'or', 'wrong', 'why', 'an', 'mean', 'does']\n",
      "18 ['pretty', 'good', \"that's\", 'look', 'looks', 'really', 'cool', 'great', 'bad', 'too']\n",
      "19 ['name', 'his', 'him', \"he's\", 'guy', 'has', 'oh', 'damn', 'from', 'who']\n",
      "20 ['an', 'or', 'any', 'there', 'way', 'would', 'use', 'interesting', 'actually', \"that's\"]\n",
      "21 ['get', 'will', 'out', 'go', 'some', 'need', 'maybe', 'keep', 'going', 'back']\n",
      "22 ['years', 'year', 'old', 'last', 'ago', 'still', 'time', 'few', 'long', 'days']\n",
      "23 ['she', 'her', 'name', \"she's\", 'girl', 'his', 'mom', 'who', 'him', 'when']\n",
      "24 [\"i'm\", 'right', 'now', 'name', 'sure', 're', 'think', 'there', 'getting', \"you're\"]\n",
      "25 ['out', 'some', 'weird', 'its', 'lol', 'when', 'from', 'also', \"that's\", 'hot']\n",
      "26 ['from', 'there', 'live', 'where', 'world', 'place', 'here', 'we', 'new', 'our']\n",
      "27 ['been', \"i've\", 'never', 'has', 'ever', 've', 'seen', 'had', 'one', 'heard']\n",
      "28 ['too', 'time', 'feel', 'really', 'bad', 'one', 'every', 'same', 'way', 'only']\n",
      "29 ['more', 'than', 'better', 'much', 'makes', 'even', 'think', 'less', 'would', 'way']\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../scripts/\"\n",
    "topics_matrix_df = pd.read_csv(base_path + (\"../data/processed_LDA_files/\" + dataset + \"_50.csv\"))\n",
    "word2idx = dict(zip(topics_matrix_df[\"words\"], range(len(topics_matrix_df[\"words\"]))))\n",
    "topics_matrix_df.drop(columns=[\"words\"], inplace=True)\n",
    "topics_matrix_df = topics_matrix_df.T\n",
    "topics_raw = topics_matrix_df.to_numpy()\n",
    "sorted_words = np.argsort(topics_raw, axis=1)\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "for i in range(topics_raw.shape[0]):\n",
    "    print(i, [idx2word[word_idx] for word_idx in sorted_words[i][::-1][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\n",
      "for\n",
      "of\n",
      "and\n",
      "on\n",
      "to\n",
      "the\n",
      "afterlife.\n",
      "[0.2003762051463127, -0.15281378291547298, -0.20729783363640308, 0.11858675349503756, -0.08425124082714319, 0.007137363776564598, 0.01972377859055996, 0.005102875952919324, -0.004639976347486178, -0.005184094111124675, -0.008481353521347046, 0.09562041610479355, -0.0773390680551529, -0.006970260292291641, 0.0005980096757411957, 0.0005573878685633341, -0.023366148273150124]\n",
      "[ 0.         -0.01104525  0.00581416 -0.01441691  0.00483219 -0.00590993\n",
      "  0.01627969 -0.09985756 -0.00693229  0.00785239  0.09768289  0.\n",
      "  0.          0.         -0.04073291 -0.00318874 -0.06766862  0.\n",
      "  0.          0.00771972  0.00675133  0.         -0.00959804 -0.00306536\n",
      "  0.         -0.00611068 -0.03786474 -0.09854012 -0.04376533  0.0013255\n",
      "  0.17779764]\n",
      "-0.12264096736907959\n",
      "-0.12264096736907962\n",
      "They got bored from haunting earth for thousands of years and ultimately moved on to the afterlife.\n"
     ]
    }
   ],
   "source": [
    "model1, _ = load_models(config)\n",
    "explainer = shap.Explainer(model1, padding=\"max_length\", truncation=True, max_length=512)\n",
    "idx = 4\n",
    "tok_sample = explainer.masker.data_transform(x[idx])[0]\n",
    "print(tok_sample)\n",
    "_, words = word_shap(tok_sample, np.zeros(len(tok_sample)))\n",
    "for tok in words:\n",
    "    if tok not in word2idx:\n",
    "        print(tok)\n",
    "print(roberta_word_vals[idx])\n",
    "print(roberta_topic_vals[idx])\n",
    "print(sum(roberta_word_vals[idx]))\n",
    "print(sum(roberta_topic_vals[idx]))\n",
    "print(x[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35931513 0.74429793 1.18468939 0.46055958 0.3257553  1.45500909\n",
      " 1.02774093 1.00907149 1.04191695 0.51910663]\n",
      "5\n",
      "[30 25 19]\n",
      "29 here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you .\n"
     ]
    }
   ],
   "source": [
    "X = roberta_topic_vals / np.sum(roberta_topic_vals, axis=1, keepdims=True)\n",
    "kmeans = KMeans(n_clusters=10).fit(X)\n",
    "clabels = kmeans.labels_\n",
    "\n",
    "diffs = np.zeros(10)\n",
    "for c in range(10):\n",
    "    diffs[c] = np.sum(np.abs(np.sum(roberta_topic_vals[clabels == c], axis=0) - np.sum(gpt2_topic_vals[clabels == c], axis=0)) / np.sum(clabels == c))\n",
    "most_diff_cluster = np.argsort(diffs)[::-1][0]\n",
    "print(diffs)\n",
    "print(most_diff_cluster)\n",
    "diff = np.abs(np.sum(roberta_topic_vals[clabels == most_diff_cluster], axis=0) - np.sum(gpt2_topic_vals[clabels == most_diff_cluster], axis=0)) / np.sum(clabels == most_diff_cluster)\n",
    "print(np.argsort(diff)[::-1][:3])\n",
    "\n",
    "for idx in np.nonzero(clabels == most_diff_cluster)[0]:\n",
    "    print(idx, x[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.18468939 1.24345609 1.34729508 1.45500909 1.50728886]\n"
     ]
    }
   ],
   "source": [
    "diffs = np.sum(np.abs(roberta_topic_vals - gpt2_topic_vals), axis=1)\n",
    "biggest_diff = np.argsort(diffs)\n",
    "print(diffs[biggest_diff[-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you .\n",
      "topic_19: -0.1553845656310764\n",
      "topic_30: -0.10612585631315596\n",
      "topic_25: -0.0935087014799168\n",
      "topic_14: -0.03646532557338526\n",
      "topic_15: -0.01593457268374577\n",
      "topic_22: 0.02807360821965514\n",
      "topic_1: 0.045695628701287475\n",
      "topic_21: 0.0492626937546679\n",
      "topic_26: 0.09268165987427164\n",
      "topic_10: 0.22321857630317818\n",
      "0.06044590473175046\n",
      "\n",
      "topic_19: -0.3872873015917917\n",
      "topic_0: 0.0\n",
      "topic_17: 0.0\n",
      "topic_16: 0.0\n",
      "topic_13: 0.0\n",
      "topic_15: 0.029227123956237402\n",
      "topic_4: 0.03711597906478719\n",
      "topic_26: 0.05891144657342237\n",
      "topic_25: 0.14575828416624173\n",
      "topic_30: 0.3104190980375279\n",
      "0.358201801776886\n"
     ]
    }
   ],
   "source": [
    "ind = 29 #biggest_diff[-1]\n",
    "print(x[ind])\n",
    "topic_names = np.array([f\"topic_{i}\" for i in range(36)])\n",
    "topic_values, sorted_names = sort_shap(roberta_topic_vals[ind], topic_names)\n",
    "for i in range(5):\n",
    "    print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "for i in range(5, 0, -1):\n",
    "    print(f\"{sorted_names[-i]}: {topic_values[-i]}\")\n",
    "print(np.sum(topic_values))\n",
    "\n",
    "print()\n",
    "topic_values, sorted_names = sort_shap(gpt2_topic_vals[ind], topic_names)\n",
    "for i in range(5):\n",
    "    print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "for i in range(5, 0, -1):\n",
    "    print(f\"{sorted_names[-i]}: {topic_values[-i]}\")\n",
    "print(np.sum(topic_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_25: 0.006110679304319723\n",
      "topic_15: 0.006897511634863846\n",
      "topic_12: 0.009089059525776176\n",
      "topic_22: 0.012098819741992079\n",
      "topic_21: 0.015178255319194547\n",
      "topic_1: 0.2552797217033191\n",
      "topic_26: 0.5835412084444859\n",
      "topic_7: 0.6627277899339665\n",
      "topic_5: 1.5791001013991892\n",
      "topic_30: 4.3985357037551385\n"
     ]
    }
   ],
   "source": [
    "topic_names = np.array([f\"topic_{i}\" for i in range(36)])\n",
    "roberta_feat_imp = np.sum(np.abs(roberta_topic_vals), axis=0)\n",
    "# gpt2_feat_imp = np.sum(np.abs(gpt2_topic_vals), axis=0)\n",
    "\n",
    "topic_values, sorted_names = sort_shap(roberta_feat_imp, topic_names)\n",
    "for i in range(5):\n",
    "    print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "for i in range(5, 0, -1):\n",
    "    print(f\"{sorted_names[-i]}: {topic_values[-i]}\")\n",
    "\n",
    "# print()\n",
    "# topic_values, sorted_names = sort_shap(gpt2_feat_imp, topic_names)\n",
    "# for i in range(5):\n",
    "#     print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "# for i in range(5, 0, -1):\n",
    "#     print(f\"{sorted_names[-i]}: {topic_values[-i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lexx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "643c8de94e1a6f14c10796bc4742cda7b866ba13d8a7cf37541d0f9d17d91a71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
