{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't initialize NVML\n"
     ]
    }
   ],
   "source": [
    "from src.utils import *\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/steinad/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/steinad/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/home/steinad/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "config = {\"dataset\": \"sst2\", \"topics\": \"lda\"}\n",
    "data_train, data_val, data_test = load_data(config)\n",
    "x = [data_test[i]['sentence'] for i in range(100)]\n",
    "# x = [xi for xi in x if len(xi.split()) > 1]\n",
    "# topics, word2idx = get_topics(config, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, word2idx = get_topics({\"dataset\": \"sst2\", \"topics\": \"lda\"}, x)\n",
    "roberta_shap_vals = load(\"shap_vals_gpt2_sst2\")\n",
    "roberta_topic_vals = load(\"topic_vals_distilroberta_sst2_lda\").reshape(-1, 31)\n",
    "roberta_word_vals = load(\"word_vals_distilroberta_sst2_lda\")\n",
    "gpt2_topic_vals = load(\"topic_vals_gpt2_sst2_lda\").reshape(-1, 31)\n",
    "gpt2_word_vals = load(\"word_vals_gpt2_sst2_lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['dull', 'nor', 'neither', 'fresh', 'far', 'every', 'original', 'goes', 'day', 'watch']\n",
      "1 ['action', 'wo', 'when', 'still', 'away', 'compelling', 'theater', 'tale', 'worthwhile', 'imagination']\n",
      "2 ['plot', 'picture', 'material', 'rather', 'boring', 'brilliant', 'mess', 'high', 'obvious', 'tired']\n",
      "3 ['acting', 'direction', 'clever', 'script', 'dialogue', 'long', 'poorly', 'music', 'screenplay', 'journey']\n",
      "4 ['energy', 'intelligence', 'cinematic', 'fascinating', 'entertaining', 'two', 'documentary', 'impressive', 'passion', 'touch']\n",
      "5 ['minutes', 'long', 'first', 'many', 'nearly', 'nothing', 'short', 'care', 'feels', 'watch']\n",
      "6 ['bland', 'without', 'dumb', 'american', 'thoroughly', 'plain', 'him', 'beauty', 'cute', 'deserves']\n",
      "7 ['better', 'worst', 'year', 'could', 'films', 'look', 'him', 'those', 'ugly', 'hollywood']\n",
      "8 ['performances', 'cast', 'full', 'performance', 'often', 'really', 'solid', 'them', 'strong', 'while']\n",
      "9 ['visual', 'own', 'beautiful', 'rich', 'kind', 'spirit', 'cast', 'part', 'see', 'style']\n",
      "10 ['made', 'could', 'hard', 'american', 'sometimes', 'ever', 'away', 'taste', 'every', 'find']\n",
      "11 ['performance', 'sweet', 'family', 'power', 'moments', 'perfect', 'feature', 'debut', 'films', 'heart']\n",
      "12 ['comic', 'real', 'could', 'intriguing', 'creepy', 'emotional', 'years', 'tale', 'intelligent', 'welcome']\n",
      "13 ['own', 'her', 'see', 'interest', 'dramatic', 'hard', 'would', 'original', 'right', 'people']\n",
      "14 ['pretty', 'romantic', 'delightful', 'often', 'adventure', 'quirky', 'find', 'surprisingly', 'same', 'fun']\n",
      "15 ['should', 'part', 'off', 'being', 'where', 'when', 'know', 'big', 'anyone', 'why']\n",
      "16 ['thriller', 'fascinating', 'psychological', 'years', 'compelling', 'both', 'ride', 'filmmaking', 'audiences', 'wild']\n",
      "17 ['seen', 've', 'big', 'had', 'screen', 'far', 'enjoyable', 'before', 'got', 'we']\n",
      "18 ['idea', 'me', 'low', 'why', 'feel', 'may', 'made', 'itself', 'watching', 'end']\n",
      "19 ['well', 'entertaining', 'amusing', 'romantic', 'drama', 'moving', 'done', 'nothing', 'worth', 'simple']\n",
      "20 ['every', 'works', 'opera', 'well', 'sense', 'soap', 'plot', 'images', 'clich', 'filmmaking']\n",
      "21 ['she', 'two', 'real', 'her', 'watching', 'character', 'hours', 'own', 'young', 'may']\n",
      "22 ['world', 'man', 'both', 'war', 'once', 'epic', 'because', 'portrait', 'emotional', 'action']\n",
      "23 ['other', 'character', 'each', 'things', 'really', 'going', 'go', 'truly', 'we', 'matter']\n",
      "24 ['see', 'interesting', 'our', 'those', 'we', 'll', 'human', 'again', 'offers', 'these']\n",
      "25 ['great', 'yet', 'actors', 'another', 'gentle', 'under', 'drama', 'look', 'such', 'de']\n",
      "26 ['special', 'fun', 'effects', 'entertainment', 'dialogue', 'flat', 'scary', 'dull', 'silly', 'violence']\n",
      "27 ['better', 'over', 're', 'thing', 'summer', 'being', 'own', 'real', 'without', 'enjoy']\n",
      "28 ['heart', 'cinema', 'down', 'whole', 'scenes', 'tale', 'acting', 'visually', 'smart', 'knows']\n",
      "29 ['how', 'audience', 'worth', 'back', 'watch', 'going', 'another', 'nothing', 'home', 'lost']\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../scripts/\"\n",
    "topics_matrix_df = pd.read_csv(base_path + (\"../data/processed_LDA_files/\" + \"sst2\" + \".csv\"))\n",
    "word2idx = dict(zip(topics_matrix_df[\"words\"], range(len(topics_matrix_df[\"words\"]))))\n",
    "topics_matrix_df.drop(columns=[\"words\"], inplace=True)\n",
    "topics_matrix_df = topics_matrix_df.T\n",
    "topics_raw = topics_matrix_df.to_numpy()\n",
    "sorted_words = np.argsort(topics_raw, axis=1)\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "for i in range(topics_raw.shape[0]):\n",
    "    print(i, [idx2word[word_idx] for word_idx in sorted_words[i][::-1][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01852864772081375, 0.0006963387131690979, 0.005582142621278763, 0.022627518512308598, -0.016558256931602955, -0.010342827066779137, 0.0244749765843153, 0.0010270774364471436, 0.007224409756335345, 0.0046737016263333235, -0.0031853812662037935, -0.009010147388008509, 0.07451375192877921, 0.03656286365267905, -0.00515490033748475, -0.12643968813283596, -0.11865620104292633, 0.13449292054936063, 0.03400407629934224]\n",
      "[-0.01233354  0.         -0.00716496 -0.00073731 -0.00252903 -0.01992763\n",
      " -0.01108731 -0.00053977  0.          0.         -0.00170561  0.02390985\n",
      " -0.00040094 -0.0164027  -0.00584376 -0.00891486  0.00028088  0.00095957\n",
      " -0.00568907 -0.00026043 -0.00095403 -0.0260686  -0.00169033 -0.00527572\n",
      " -0.01523971  0.00248917  0.00110983  0.          0.         -0.00030594\n",
      "  0.18938297]\n",
      "0.075061023235321\n",
      "0.07506102323532102\n",
      "lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "print(roberta_word_vals[idx])\n",
    "print(roberta_topic_vals[idx])\n",
    "print(sum(roberta_word_vals[idx]))\n",
    "print(sum(roberta_topic_vals[idx]))\n",
    "print(x[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35931513 0.74429793 1.18468939 0.46055958 0.3257553  1.45500909\n",
      " 1.02774093 1.00907149 1.04191695 0.51910663]\n",
      "5\n",
      "[30 25 19]\n",
      "29 here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you .\n"
     ]
    }
   ],
   "source": [
    "X = roberta_topic_vals / np.sum(roberta_topic_vals, axis=1, keepdims=True)\n",
    "kmeans = KMeans(n_clusters=10).fit(X)\n",
    "clabels = kmeans.labels_\n",
    "\n",
    "diffs = np.zeros(10)\n",
    "for c in range(10):\n",
    "    diffs[c] = np.sum(np.abs(np.sum(roberta_topic_vals[clabels == c], axis=0) - np.sum(gpt2_topic_vals[clabels == c], axis=0)) / np.sum(clabels == c))\n",
    "most_diff_cluster = np.argsort(diffs)[::-1][0]\n",
    "print(diffs)\n",
    "print(most_diff_cluster)\n",
    "diff = np.abs(np.sum(roberta_topic_vals[clabels == most_diff_cluster], axis=0) - np.sum(gpt2_topic_vals[clabels == most_diff_cluster], axis=0)) / np.sum(clabels == most_diff_cluster)\n",
    "print(np.argsort(diff)[::-1][:3])\n",
    "\n",
    "for idx in np.nonzero(clabels == most_diff_cluster)[0]:\n",
    "    print(idx, x[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.18468939 1.24345609 1.34729508 1.45500909 1.50728886]\n"
     ]
    }
   ],
   "source": [
    "diffs = np.sum(np.abs(roberta_topic_vals - gpt2_topic_vals), axis=1)\n",
    "biggest_diff = np.argsort(diffs)\n",
    "print(diffs[biggest_diff[-5:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here 's a british flick gleefully unconcerned with plausibility , yet just as determined to entertain you .\n",
      "topic_19: -0.1553845656310764\n",
      "topic_30: -0.10612585631315596\n",
      "topic_25: -0.0935087014799168\n",
      "topic_14: -0.03646532557338526\n",
      "topic_15: -0.01593457268374577\n",
      "topic_22: 0.02807360821965514\n",
      "topic_1: 0.045695628701287475\n",
      "topic_21: 0.0492626937546679\n",
      "topic_26: 0.09268165987427164\n",
      "topic_10: 0.22321857630317818\n",
      "0.06044590473175046\n",
      "\n",
      "topic_19: -0.3872873015917917\n",
      "topic_0: 0.0\n",
      "topic_17: 0.0\n",
      "topic_16: 0.0\n",
      "topic_13: 0.0\n",
      "topic_15: 0.029227123956237402\n",
      "topic_4: 0.03711597906478719\n",
      "topic_26: 0.05891144657342237\n",
      "topic_25: 0.14575828416624173\n",
      "topic_30: 0.3104190980375279\n",
      "0.358201801776886\n"
     ]
    }
   ],
   "source": [
    "ind = 29 #biggest_diff[-1]\n",
    "print(x[ind])\n",
    "topic_names = np.array([f\"topic_{i}\" for i in range(36)])\n",
    "topic_values, sorted_names = sort_shap(roberta_topic_vals[ind], topic_names)\n",
    "for i in range(5):\n",
    "    print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "for i in range(5, 0, -1):\n",
    "    print(f\"{sorted_names[-i]}: {topic_values[-i]}\")\n",
    "print(np.sum(topic_values))\n",
    "\n",
    "print()\n",
    "topic_values, sorted_names = sort_shap(gpt2_topic_vals[ind], topic_names)\n",
    "for i in range(5):\n",
    "    print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "for i in range(5, 0, -1):\n",
    "    print(f\"{sorted_names[-i]}: {topic_values[-i]}\")\n",
    "print(np.sum(topic_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_18: 1.1927641473527824\n",
      "topic_28: 1.2792578255936047\n",
      "topic_10: 1.4127325231550543\n",
      "topic_13: 1.6853790241062112\n",
      "topic_12: 1.6970660445412045\n",
      "topic_7: 2.672635190010829\n",
      "topic_25: 2.951807518542655\n",
      "topic_21: 2.9736445350846377\n",
      "topic_15: 2.9963853319057185\n",
      "topic_30: 24.913793117689515\n",
      "\n",
      "topic_18: 0.8932053637634415\n",
      "topic_10: 0.9632376007580045\n",
      "topic_12: 1.4546492131042734\n",
      "topic_23: 1.477474603109051\n",
      "topic_3: 1.5033561471434458\n",
      "topic_27: 2.491718571442825\n",
      "topic_16: 2.4973391853080935\n",
      "topic_29: 2.5416982269866084\n",
      "topic_15: 2.722013871904929\n",
      "topic_30: 19.318108668372073\n"
     ]
    }
   ],
   "source": [
    "roberta_feat_imp = np.sum(np.abs(roberta_topic_vals), axis=0)\n",
    "gpt2_feat_imp = np.sum(np.abs(gpt2_topic_vals), axis=0)\n",
    "\n",
    "topic_values, sorted_names = sort_shap(roberta_feat_imp, topic_names)\n",
    "for i in range(5):\n",
    "    print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "for i in range(5, 0, -1):\n",
    "    print(f\"{sorted_names[-i]}: {topic_values[-i]}\")\n",
    "\n",
    "print()\n",
    "topic_values, sorted_names = sort_shap(gpt2_feat_imp, topic_names)\n",
    "for i in range(5):\n",
    "    print(f\"{sorted_names[i]}: {topic_values[i]}\")\n",
    "for i in range(5, 0, -1):\n",
    "    print(f\"{sorted_names[-i]}: {topic_values[-i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lexx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "643c8de94e1a6f14c10796bc4742cda7b866ba13d8a7cf37541d0f9d17d91a71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
